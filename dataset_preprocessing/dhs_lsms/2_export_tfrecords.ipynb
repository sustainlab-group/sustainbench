{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Pre-requisites\" data-toc-modified-id=\"Pre-requisites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Pre-requisites</a></span></li><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Instructions</a></span></li><li><span><a href=\"#Imports-and-Constants\" data-toc-modified-id=\"Imports-and-Constants-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports and Constants</a></span></li><li><span><a href=\"#Constants\" data-toc-modified-id=\"Constants-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Constants</a></span></li><li><span><a href=\"#Export-Images\" data-toc-modified-id=\"Export-Images-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Export Images</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "Register a Google account at [https://code.earthengine.google.com](https://code.earthengine.google.com). This process may take a couple of days. Without registration, the `ee.Initialize()` command below will throw an error message.\n",
    "\n",
    "Create the DHS and LSMS labels files. See `1_create_dhs_labels.R` and `1_create_lsms_labels.R`.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook exports satellite image composites from Google Earth Engine. The images are saved in gzipped TFRecord format (`*.tfrecord.gz`). The exported images take up a significant amount of storage space. Before exporting, make sure you have enough storage space.\n",
    "\n",
    "In this project, we download satellite images corresponding to two different datasets:\n",
    "\n",
    "- **DHS**: TODO clusters from DHS surveys, for which we predict cross-sectional (*i.e.*, static in time) cluster-level asset wealth\n",
    "- **LSMS**: TODO clusters from LSMS surveys, for which we predict changes in cluster-level asset wealth over time\n",
    "\n",
    "*Note*: The DHS numbers are for all locations, including locations that were later excluded due to missing imagery.\n",
    "\n",
    "|       | Storage   | Expected Export Time\n",
    "|-------|-----------|---------------------\n",
    "| DHS   | ~97.7 GiB | ~48h\n",
    "| LSMS  |  ~2.5 GiB | ~10h\n",
    "\n",
    "By default, this notebook exports images to Google Drive. If you instead prefer to export images to Google Cloud Storage (GCS), change the `EXPORT` constant below to `'gcs'` and set `BUCKET` to the desired GCS bucket name. The images are exported to the following locations:\n",
    "\n",
    "|       | Google Drive (default)              | GCS\n",
    "|-------|:------------------------------------|:---\n",
    "| DHS   | `sustainbench_dhs_tfrecords_raw/`   | `{BUCKET}/sustainbench_dhs_tfrecords_raw/`\n",
    "| LSMS  | `sustainbench_lsms_tfrecords_raw/`  | `{BUCKET}/sustainbench_lsms_tfrecords_raw/`\n",
    "\n",
    "Once the images have finished exporting, download the exported TFRecord files from Google Drive to the following folders:\n",
    "\n",
    "- DHS: `dhs_lsms/dhs_tfrecords_raw/`\n",
    "- LSMS: `dhs_lsms/lsms_tfrecords_raw/`\n",
    "\n",
    "After downloading the TFRecord files, these directories should look as follows, where each `YYY` depends on the `CHUNK_SIZE` parameter used:\n",
    "\n",
    "```\n",
    "dhs_lsms/dhs_tfrecords_raw/\n",
    "    AL_2008_clust001_toYYY_of450.tfrecord.gz\n",
    "    ...\n",
    "    ZW_2015_clustYYY_to400_of400.tfrecord.gz\n",
    "dhs_lsms/lsms_tfrecords_raw/\n",
    "    TODO\n",
    "```\n",
    "\n",
    "After finishing this notebook, move on to [3_process_tfrecords.ipynb](./3_process_tfrecords.ipynb) for next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change directory to repo root, and verify\n",
    "# %cd '../'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "import ee_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the Earth Engine API, you must perform a one-time authentication that authorizes access to Earth Engine on behalf of your Google account you registered at [https://code.earthengine.google.com](https://code.earthengine.google.com). The authentication process saves a credentials file to `$HOME/.config/earthengine/credentials` for future use.\n",
    "\n",
    "The command `ee.Authenticate()` runs the authentication process. Once you successfully authenticate, you should not need to authenticate again in the future, unless you delete the credentials file. If you do not authenticate, the subsequent `ee.Initialize()` command will fail.\n",
    "\n",
    "For more information, see [https://developers.google.com/earth-engine/python_install-conda.html](https://developers.google.com/earth-engine/python_install-conda.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # if already authenticated, can directly intiialize the Earth Engine API\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    # otherwise, authenticate first, then initialize\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ADAPT THESE PARAMETERS ==========\n",
    "\n",
    "# To export to Google Drive, uncomment the next 2 lines\n",
    "EXPORT = 'drive'\n",
    "BUCKET = None\n",
    "\n",
    "# To export to Google Cloud Storage (GCS), uncomment the next 2 lines\n",
    "# and set the bucket to the desired bucket name\n",
    "# EXPORT = 'gcs'\n",
    "# BUCKET = 'mybucket'\n",
    "\n",
    "# export location parameters\n",
    "DHS_EXPORT_FOLDER = 'sustainbench_dhs_tfrecords_raw'\n",
    "LSMS_EXPORT_FOLDER = 'sustainbench_lsms_tfrecords_raw'\n",
    "\n",
    "# CHUNK_SIZE determines how many records (images) are included in each TFRecord file\n",
    "# per DHS survey. Set CHUNK_SIZE to None to export a single TFRecord file per survey.\n",
    "# However, sometimes this may fail by exceeding Google Earth Engine memory limits.\n",
    "# In that case, decrease CHUNK_SIZE by a factor of 10 each time (to as small as 1)\n",
    "# for Google Earth Engine to stop reporting memory errors.\n",
    "CHUNK_SIZE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DO NOT MODIFY THESE ==========\n",
    "\n",
    "# input data paths\n",
    "DHS_CSV_PATH = 'output_labels/dhs_merged.csv'\n",
    "LSMS_CSV_PATH = 'output_labels/lsms.csv'\n",
    "\n",
    "# band names\n",
    "MS_BANDS = ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TEMP1']\n",
    "\n",
    "# image parameters\n",
    "PROJECTION = 'EPSG:3857'  # see https://epsg.io/3857\n",
    "SCALE = 30                # export resolution: 30m/px\n",
    "EXPORT_TILE_RADIUS = 127  # image dimension = (2*EXPORT_TILE_RADIUS) + 1 = 255px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_images(df: pd.DataFrame,\n",
    "                  country: str,\n",
    "                  year: int,\n",
    "                  export_folder: str,\n",
    "                  chunk_size: Optional[int] = None\n",
    "                  ) -> dict[tuple[str, str, int, int], ee.batch.Task]:\n",
    "    '''\n",
    "    Args\n",
    "    - df: pd.DataFrame, contains columns ['lat', 'lon', 'country_code', 'year']\n",
    "    - country: str, together with `year` determines the survey to export\n",
    "    - year: int, together with `country` determines the survey to export\n",
    "    - export_folder: str, name of folder for export\n",
    "    - chunk_size: int, optionally set a limit to the # of images exported per TFRecord file\n",
    "        - set to a small number (<= 50) if Google Earth Engine reports memory errors\n",
    "\n",
    "    Returns: dict, maps task name tuple (export_folder, country, year, chunk) to ee.batch.Task\n",
    "    '''\n",
    "    subset_df = df[(df['country_code'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "    if chunk_size is None:\n",
    "        chunk_size = len(subset_df)\n",
    "    num_chunks = int(math.ceil(len(subset_df) / chunk_size))\n",
    "    tasks = {}\n",
    "\n",
    "    id_max = subset_df['cluster_id'].max()\n",
    "    digits_in_id = len(str(id_max))\n",
    "    id_template = '{:0' + str(digits_in_id) + 'd}'\n",
    "    id_max = id_template.format(id_max)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_slice = slice(i * chunk_size, (i+1) * chunk_size - 1)  # df.loc[] is inclusive\n",
    "        fc = ee_utils.df_to_fc(subset_df.loc[chunk_slice, :])\n",
    "        start_date, end_date = ee_utils.surveyyear_to_range(year)\n",
    "\n",
    "        # create 3-year Landsat composite image\n",
    "        imgcol = ee_utils.LandsatSR(start_date=start_date, end_date=end_date).merged\n",
    "        imgcol = imgcol.map(ee_utils.mask_qaclear).select(MS_BANDS)\n",
    "        img = imgcol.median()\n",
    "\n",
    "        # add nightlights, latitude, and longitude bands\n",
    "        img = ee_utils.add_latlon(img)\n",
    "        img = img.addBands(ee_utils.composite_nl(year))\n",
    "\n",
    "        # create unique filename for export\n",
    "        stop = min(chunk_slice.stop, len(subset_df) - 1)\n",
    "        id_start = subset_df.loc[chunk_slice.start, 'cluster_id']\n",
    "        id_end = subset_df.loc[stop, 'cluster_id']\n",
    "        assert id_end >= id_start\n",
    "        id_start = id_template.format(id_start)\n",
    "        id_end = id_template.format(id_end)\n",
    "        fname = f'- {country}_{year}_clust{id_start}_to{id_end}_of{id_max}'\n",
    "        print(fname)\n",
    "\n",
    "        tasks[(export_folder, country, year, i)] = ee_utils.get_array_patches(\n",
    "            img=img, scale=SCALE, ksize=EXPORT_TILE_RADIUS,\n",
    "            points=fc, export=EXPORT,\n",
    "            prefix=export_folder, fname=fname,\n",
    "            bucket=BUCKET)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks: dict[tuple[str, str, int, int], ee.batch.Task] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_df = pd.read_csv(DHS_CSV_PATH, float_precision='high', index_col=False).sort_values('DHSID_EA')\n",
    "\n",
    "# exclude adm1* because of NaNs\n",
    "dhs_df = dhs_df[['DHSID_EA', 'country_code', 'cluster_id', 'urban', 'lat', 'lon', 'year']]\n",
    "display(dhs_df.head())\n",
    "\n",
    "dhs_surveys = list(dhs_df.groupby(['country_code', 'year']).groups.keys())\n",
    "for country, year in dhs_surveys:\n",
    "    print(country, year)\n",
    "    new_tasks = export_images(\n",
    "        df=dhs_df, country=country, year=year,\n",
    "        export_folder=DHS_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsms_df = pd.read_csv(LSMS_CSV_PATH, float_precision='high', index_col=False)\n",
    "lsms_surveys = list(lsms_df.groupby(['country', 'year']).groups.keys())\n",
    "\n",
    "for country, year in lsms_surveys:\n",
    "    new_tasks = export_images(\n",
    "        df=lsms_df, country=country, year=year,\n",
    "        export_folder=LSMS_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on the status of each export task at [https://code.earthengine.google.com/](https://code.earthengine.google.com/), or run the following cell which checks every minute. Once all tasks have completed, download the DHS TFRecord files to `data/dhs_tfrecords_raw/`, DHSNL TFRecord files to `data/dhsnl_tfrecords_raw/`, and LSMS TFRecord files to `data/lsms_tfrecords_raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_utils.wait_on_tasks(tasks, poll_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sustainbench_create] *",
   "language": "python",
   "name": "conda-env-sustainbench_create-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
