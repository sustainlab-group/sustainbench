{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuwlACzqv9gd",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Pre-requisites\" data-toc-modified-id=\"Pre-requisites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Pre-requisites</a></span></li><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Instructions</a></span></li><li><span><a href=\"#Imports-and-Constants\" data-toc-modified-id=\"Imports-and-Constants-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports and Constants</a></span></li><li><span><a href=\"#Validate-and-Split-Exported-TFRecords\" data-toc-modified-id=\"Validate-and-Split-Exported-TFRecords-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Validate and Split Exported TFRecords</a></span></li><li><span><a href=\"#Calculate-Mean-and-Std-Dev-for-Each-Band\" data-toc-modified-id=\"Calculate-Mean-and-Std-Dev-for-Each-Band-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Calculate Mean and Std-Dev for Each Band</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80vAyegSv9gl"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Go through the [`2_export_tfrecords.ipynb`](./2_export_tfrecords.ipynb) notebook.\n",
    "\n",
    "Before running this notebook, you should have the following directory structure:\n",
    "\n",
    "```\n",
    "dhs_lsms/dhs_tfrecords_raw/\n",
    "    {DHSEA_ID}__to__{DHSEA_ID}.tfrecord.gz\n",
    "    ...\n",
    "dhs_lsms/lsms_tfrecords_raw/\n",
    "    TODO\n",
    "```\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook processes the exported TFRecords as follows:\n",
    "1. Verifies that the fields in the TFRecords match the original CSV files.\n",
    "2. Splits each monolithic TFRecord file exported from Google Earth Engine into one numpy file per record.\n",
    "3. TODO\n",
    "\n",
    "After running this notebook, you should have 2 new folders (`dhs_npzs` and `lsms_npzs`):\n",
    "\n",
    "```\n",
    "dhs_lsms/dhs_npzs/\n",
    "    {survey_name}/\n",
    "        {DHSID_EA}.npz\n",
    "dhs_lsms/lsms_npzs/\n",
    "    {survey_name}/\n",
    "        {TODO}.npz\n",
    "```\n",
    "\n",
    "The amount of storage required is\n",
    "\n",
    "|       | Storage   | Expected Processing Time\n",
    "|-------|-----------|-------------------------\n",
    "| DHS   | ~94.7 GiB | ~8h\n",
    "| LSMS  |  ~??? GiB | ~TODOh\n",
    "\n",
    "This notebook also calculates the mean and standard deviation of each band for the DHS images.\n",
    "\n",
    "It may be convenient to directly run this notebook on Google Colab, especially if the TFRecords were exported to Google Drive instead of Google Cloud Storage. When doing so, uncomment the cell below which starts with\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoIQ9vjav9go"
   },
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0NLhj2NsVqF"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dO84BMNTxYU-",
    "outputId": "39824d4a-f720-4512-85dd-4f64cc197b7f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/drive', force_remount=True)\n",
    "# %cd '/drive/MyDrive/sustainbench'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQ8IEERrv9gp",
    "outputId": "2e83801e-7bd0-4b13-8f96-60cd94b110b5"
   },
   "outputs": [],
   "source": [
    "# change directory to repo root, and verify\n",
    "# %cd '../dataset_preprocessing'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQgfpSQLv9gq",
    "outputId": "3cb38bf2-ce11-4905-c59e-a95c30d6c06c"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from collections import namedtuple\n",
    "from glob import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vr7bQPpxv9gr"
   },
   "outputs": [],
   "source": [
    "REQUIRED_BANDS = [\n",
    "    'BLUE', 'GREEN', 'RED', 'SWIR1', 'SWIR2', 'TEMP1', 'NIR', 'NIGHTLIGHTS']\n",
    "\n",
    "DHS_EXPORT_FOLDER = 'dhs_tfrecords_raw'\n",
    "DHS_PROCESSED_FOLDER = 'dhs_npzs'\n",
    "DHS_INPUT_CSV_PATH = 'output_labels/dhs_merged.csv'  # CSV mapping DHSID_EA to ['lat', 'lon', labels]\n",
    "DHS_FINAL_CSV_PATH = 'output_labels/dhs_final_labels.csv'\n",
    "\n",
    "LSMS_EXPORT_FOLDER = 'lsms_tfrecords_raw'\n",
    "LSMS_PROCESSED_FOLDER = 'lsms_npzs'\n",
    "LSMS_INPUT_CSV_PATH = 'output_labels/lsms.csv'  # TODO\n",
    "LSMS_FINAL_CSV_PATH = 'output_labels/lsms_final_labels.csv'  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12jeEXcPv9gs"
   },
   "source": [
    "## Validate and Split Exported TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3NpKPzZv9gs"
   },
   "outputs": [],
   "source": [
    "def process_dataset(csv_path: str, index_col: str, input_dir: str, processed_dir: str,\n",
    "                    log_path: str) -> None:\n",
    "    '''\n",
    "    Args\n",
    "    - csv_path: str, path to CSV of DHS or LSMS clusters\n",
    "    - index_col: str, name of column in CSV to use as unique index\n",
    "    - input_dir: str, path to TFRecords exported from Google Earth Engine\n",
    "    - processed_dir: str, folder where to save processed TFRecords\n",
    "    - log_path: str, path to log file\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, float_precision='high', index_col=index_col)\n",
    "\n",
    "    # cast float64 => float32 and str => bytes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        elif df[col].dtype == object:  # pandas uses 'object' type for str\n",
    "            df[col] = df[col].astype(bytes)\n",
    "\n",
    "    df['survey'] = df.index.str[:10]  # TODO: check if this works with LSMS\n",
    "    surveys = list(df.groupby('survey').groups.keys())\n",
    "\n",
    "    if os.path.exists(log_path):\n",
    "        log = pd.read_csv(log_path, index_col=index_col)\n",
    "    else:\n",
    "        log = pd.DataFrame(index=pd.Index([], name=index_col),\n",
    "                           columns=['status'])\n",
    "\n",
    "    # use this list to track any surveys that have already been processed\n",
    "    # (this is useful for processing surveys in batches)\n",
    "    PROCESSED_SURVEYS = []\n",
    "\n",
    "    pbar = tqdm()\n",
    "    for i, survey in enumerate(surveys):\n",
    "        tqdm.write(f'Processing: {survey}, ({i+1} / {len(surveys)})')\n",
    "\n",
    "        if survey in PROCESSED_SURVEYS:\n",
    "            tqdm.write(f'- Already processed')\n",
    "            continue\n",
    "\n",
    "        tfrecord_paths = glob(os.path.join(input_dir, survey + '*'))\n",
    "        if len(tfrecord_paths) == 0:\n",
    "            tqdm.write(f'- No TFRecords found')\n",
    "            continue\n",
    "\n",
    "        out_dir = os.path.join(processed_dir, survey)\n",
    "        subset_df = df[df['survey'] == survey].sort_index()\n",
    "        log_new = validate_and_split_tfrecords(\n",
    "            tfrecord_paths=tfrecord_paths, out_dir=out_dir, df=subset_df,\n",
    "            pbar=pbar)\n",
    "        log = pd.concat([log, log_new], verify_integrity=True)\n",
    "        log.to_csv(log_path)\n",
    "\n",
    "\n",
    "DEFAULT = np.nan * np.ones(255**2)\n",
    "\n",
    "def parse_record(ex, index_col: str):\n",
    "    keys_to_features = {\n",
    "        band: tf.io.FixedLenFeature(shape=[255**2], dtype=tf.float32,\n",
    "                                    default_value=DEFAULT)\n",
    "        for band in REQUIRED_BANDS\n",
    "    }\n",
    "    keys_to_features.update({\n",
    "        'cluster_id': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'lat': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'lon': tf.io.FixedLenFeature([], tf.float32),\n",
    "        index_col: tf.io.FixedLenFeature([], tf.string),\n",
    "    })\n",
    "    ex = tf.io.parse_single_example(ex, keys_to_features)\n",
    "    ex['img'] = tf.stack([\n",
    "        tf.reshape(ex[band], [255, 255])\n",
    "        for band in REQUIRED_BANDS\n",
    "    ])\n",
    "    for band in REQUIRED_BANDS:\n",
    "        del ex[band]\n",
    "    return ex\n",
    "\n",
    "\n",
    "def validate_and_split_tfrecords(\n",
    "        tfrecord_paths: Iterable[str],\n",
    "        index_col: str,\n",
    "        out_dir: str,\n",
    "        df: pd.DataFrame,\n",
    "        pbar: tqdm | None = None\n",
    "        ) -> None:\n",
    "    '''Validates and splits a list of exported TFRecord files (for a\n",
    "    given country-year survey) into individual TFrecords, one per cluster.\n",
    "\n",
    "    \"Validating\" a TFRecord comprises of 2 parts\n",
    "    1) verifying that it contains the required bands\n",
    "    2) verifying that its other features match the values from the dataset CSV\n",
    "\n",
    "    Args\n",
    "    - tfrecord_paths: list of str, paths to exported TFRecords files\n",
    "    - index_col: str, name of column in CSV to use as unique index\n",
    "    - out_dir: str, path to dir to save processed individual TFRecords\n",
    "    - df: pd.DataFrame, index is DHSID_EA\n",
    "    - pbar: tqdm, optional progress bar\n",
    "    '''\n",
    "    processed = []  # GEE exported, all good to go!\n",
    "    missing_bands = []  # GEE exported, but missing some bands\n",
    "    missing_labels = []  # GEE exported, but missing labels in CSV\n",
    "    no_record = []  # no TFRecord found\n",
    "\n",
    "    # create a progress bar if not given one already\n",
    "    should_close_pbar = False\n",
    "    if pbar is None:\n",
    "        pbar = tqdm()\n",
    "        should_close_pbar = True\n",
    "    pbar.reset(total=len(df))\n",
    "\n",
    "    # flag for whether to create output directory\n",
    "    should_make_out_dir = not os.path.exists(out_dir)\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(tfrecord_paths, compression_type='GZIP')\n",
    "    ds = ds.map(lambda ex: parse_record(ex, index_col))\n",
    "    for record in ds.as_numpy_iterator():\n",
    "        uniq_id = record[index_col].decode()\n",
    "        if record_id not in df.index:\n",
    "            missing_labels.append(uniq_id)\n",
    "        elif np.isnan(record['img']).any():\n",
    "            missing_bands.append(uniq_id)\n",
    "        else:\n",
    "            # optional: compare feature map values against CSV values\n",
    "            csv_feats = df.loc[uniq_id, :].to_dict()\n",
    "            for col, val in csv_feats.items():\n",
    "                if col in record and record[col] != val:\n",
    "                    tqdm.write(f'- {uniq_id}: record[{col}] = {record[col]}, '\n",
    "                               f'CSV val = {val}')\n",
    "\n",
    "            if should_make_out_dir:\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                should_make_out_dir = False\n",
    "\n",
    "            save_path = os.path.join(out_dir, uniq_id)\n",
    "            np.savez_compressed(save_path, x=record['img'])\n",
    "            processed.append(uniq_id)\n",
    "        pbar.update(1)\n",
    "\n",
    "    if should_close_pbar:\n",
    "        pbar.close()\n",
    "\n",
    "    seen = missing_bands + missing_labels + processed\n",
    "    expected = df.index.to_numpy()\n",
    "    no_record = np.setdiff1d(expected, seen)\n",
    "\n",
    "    log = pd.concat([\n",
    "        pd.DataFrame(index=pd.Index(arr, name=index_col),\n",
    "                     data={'status': status})\n",
    "        for status, arr in [\n",
    "            ('processed', processed),\n",
    "            ('missing_bands', missing_bands),\n",
    "            ('missing_labels', missing_labels),\n",
    "            ('no_record', no_record)\n",
    "        ]\n",
    "    ], verify_integrity=True)\n",
    "    return log\n",
    "\n",
    "def check_log(csv_path: str, index_col: str, log_path: str) -> None:\n",
    "    '''Validates and splits a list of exported TFRecord files (for a\n",
    "    given country-year survey) into individual TFrecords, one per cluster.\n",
    "\n",
    "    \"Validating\" a TFRecord comprises of 2 parts\n",
    "    1) verifying that it contains the required bands\n",
    "    2) verifying that its other features match the values from the dataset CSV\n",
    "\n",
    "    Args\n",
    "    - csv_path: str, path to labels CSV, columns include [index_col]\n",
    "    - index_col: str, name of column in CSV to use as unique index\n",
    "    - log_path: str, path to log CSV, columns are [index_col, 'status']\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, index_col=False)\n",
    "    df.set_index(index_col, inplace=True, verify_integrity=True)\n",
    "    df['survey'] = df.index.str[:10]  # TODO: check if this works with LSMS\n",
    "    labeled_surveys = df['survey'].unique()\n",
    "\n",
    "    log = pd.read_csv(log_path, index_col=False)\n",
    "    log.set_index(index_col, inplace=True, verify_integrity=True)\n",
    "    log['survey'] = log.index.str[:10]  # TODO: check if this works with LSMS\n",
    "    logged_surveys = log['survey'].unique()\n",
    "    print('logged surveys not in labels:',\n",
    "          sorted(np.setdiff1d(logged_surveys, labeled_surveys)))\n",
    "    print('labeled surveys not in log:',\n",
    "          sorted(np.setdiff1d(labeled_surveys, logged_surveys)))\n",
    "\n",
    "    # get list of processed npzs which aren't in the labels CSV\n",
    "    all_labeled_clusters = df.index\n",
    "    all_processed_clusters = log[log['status'] == 'processed'].index\n",
    "    unlabeled_npzs = sorted(set(all_processed_clusters) - set(all_labeled_clusters))\n",
    "    print('num npzs missing labels:', len(unlabeled_npzs))\n",
    "    print('npzs missing labels:', unlabeled_npzs)\n",
    "\n",
    "    # for each survey in the log, check that the uniq_id's from the labels CSV\n",
    "    # are all in the log\n",
    "    for survey in logged_surveys:\n",
    "        label_ids = df.loc[df['survey'] == survey].index\n",
    "        log_ids = log.loc[log['survey'] == survey].index\n",
    "        assert label_ids.isin(log_ids).all()\n",
    "\n",
    "    # use some jupyter magic to get a list of empty directories\n",
    "    # only surveys where no images were properly processed should be empty\n",
    "    # TODO: update for LSMS\n",
    "    empty_dirs = !find dhs_npzs -type d -empty\n",
    "    for empty_dir in empty_dirs:\n",
    "        survey = empty_dir.split('/')[1]\n",
    "        if survey in logged_surveys:\n",
    "            assert ((log['survey'] == survey) &\n",
    "                    (log['status'] == 'processed')).sum() == 0\n",
    "            print(f'survey {survey} has nothing processed')\n",
    "        elif survey not in labeled_surveys:\n",
    "            print(f'survey {survey} should not exist')\n",
    "\n",
    "    # check that every processed image was actually in the labels CSV\n",
    "\n",
    "    print('=== breakdown by status ===')\n",
    "    display(log.groupby('status').size())\n",
    "\n",
    "    incomplete_surveys = log.loc[log['status'] != 'processed', 'survey'].unique()\n",
    "    not_processed_sizes = (\n",
    "        log.loc[log['survey'].isin(incomplete_surveys)]\n",
    "        .groupby(['survey', 'status'])\n",
    "        .size()\n",
    "    )\n",
    "    display(not_processed_sizes.unstack().astype(pd.Int64Dtype()))\n",
    "\n",
    "    empty_surveys = log.groupby('survey').filter(lambda df: (df['status'] != 'processed').all())\n",
    "    print('surveys without any processed:', empty_surveys['survey'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rICNXVSS3NTZ"
   },
   "outputs": [],
   "source": [
    "ds = process_dataset(\n",
    "    csv_path=DHS_INPUT_CSV_PATH,\n",
    "    index_col='DHSID_EA',\n",
    "    input_dir=DHS_EXPORT_FOLDER,\n",
    "    processed_dir=DHS_PROCESSED_FOLDER,\n",
    "    log_path='dhs_tfrecords_export_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1tb230fIKk7"
   },
   "outputs": [],
   "source": [
    "check_log(csv_path=DHS_INPUT_CSV_PATH,\n",
    "          index_col='DHSID_EA',\n",
    "          log_path='dhs_tfrecords_export_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jC820ugVN1f"
   },
   "outputs": [],
   "source": [
    "ds = process_dataset(\n",
    "    csv_path=LSMS_INPUT_CSV_PATH,\n",
    "    index_col=TODO,  # TODO\n",
    "    input_dir=LSMS_EXPORT_FOLDER,\n",
    "    processed_dir=LSMS_PROCESSED_FOLDER,\n",
    "    log_path='lsms_tfrecords_export_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijWmOpSLVUSZ"
   },
   "source": [
    "## Verify images\n",
    "\n",
    "Randomly sample 20 `.npz` files and plot them vs. expected images from Google Earth Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFC4FJY-VTiN"
   },
   "outputs": [],
   "source": [
    "import ee  # earthengine\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "try:\n",
    "    # if already authenticated, can directly intiialize the Earth Engine API\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    # otherwise, authenticate first, then initialize\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZWGPugUXTbq"
   },
   "outputs": [],
   "source": [
    "def ee_viz(lat: float, lon: float, year: int) -> None:\n",
    "    \"\"\"Displays 255x255px Landsat 5/7/8 surface reflectance image tiles\n",
    "    (3-year median composite tile) centered on the given lat/lon coordinates.\n",
    "\n",
    "    This image will not be as \"clean\" as the GEE-exported composites because\n",
    "    here we do not do any fancy cloud masking / QA control.\n",
    "    \"\"\"\n",
    "    # get 255x255px box around (lat, lon)\n",
    "    res = 30  # meters per pixel\n",
    "    radius = 255 / 2.0  # radius of image in pixels\n",
    "    pt = ee.Geometry.Point([lon, lat])\n",
    "    roi = pt.buffer(radius * res).bounds()\n",
    "\n",
    "    SatParam = namedtuple(\n",
    "        'SatParam', ['col_name', 'min_year', 'max_year', 'rgb_bands', 'scale'])\n",
    "    params = {\n",
    "        'Landsat 5': SatParam(\n",
    "            col_name='LANDSAT/LT05/C01/T1_SR', min_year=1984, max_year=2012,\n",
    "            rgb_bands=['B3', 'B2', 'B1'], scale=0.0001),\n",
    "        'Landsat 7': SatParam(\n",
    "            col_name='LANDSAT/LE07/C01/T1_SR', min_year=1999, max_year=2021,\n",
    "            rgb_bands=['B3', 'B2', 'B1'], scale=0.0001),\n",
    "        'Landsat 8': SatParam(\n",
    "            col_name='LANDSAT/LC08/C01/T1_SR', min_year=2013, max_year=2021,\n",
    "            rgb_bands=['B4', 'B3', 'B2'], scale= 0.0001)\n",
    "    }\n",
    "\n",
    "    # these values empirically seem to work well for L7 and L8 images\n",
    "    vis_params = {\n",
    "        'min': 0,     # becomes 0 in RGB\n",
    "        'max': 0.35,  # becomes 255 in RGB\n",
    "        # 'gamma': 2.5  # set between [1, 2.5] to match your own aesthetic\n",
    "    }\n",
    "\n",
    "    img_urls = {}\n",
    "    for name, sat in params.items():\n",
    "        if (year + 1 < sat.min_year) or (year - 1 > sat.max_year):\n",
    "            continue\n",
    "\n",
    "        # get Landsat surface reflectance image\n",
    "        start_year = max(sat.min_year, year - 1)\n",
    "        end_year = min(sat.max_year, year + 1)\n",
    "        start = f'{start_year}-01-01'\n",
    "        end = f'{end_year}-12-31'\n",
    "        img = (\n",
    "            ee.ImageCollection(sat.col_name)\n",
    "            .filterDate(start, end)\n",
    "            .select(sat.rgb_bands)\n",
    "            .median()\n",
    "            .multiply(sat.scale)\n",
    "        )\n",
    "\n",
    "        # Create a URL to the image, and display it\n",
    "        url = img.getThumbUrl(\n",
    "            {**vis_params, 'dimensions': 255, 'region': roi})\n",
    "        print(name)\n",
    "        display(Image(url=url))\n",
    "\n",
    "\n",
    "def npz_viz(npz_path: str) -> None:\n",
    "    '''Visualizes the RGB and NL bands of a satellite image stored in a\n",
    "    .npz file.\n",
    "\n",
    "    Note: GEE images are exported with (0,0) = lower-left corner. By default,\n",
    "    matplotlib's plt.imshow() and PIL.Image assume (0,0) = upper-left corner.\n",
    "    '''\n",
    "    with open(npz_path, 'rb') as f:\n",
    "        img = np.load(f)['x']\n",
    "    rgb = np.stack([img[2], img[1], img[0]], axis=2)\n",
    "    nl = img[-1]\n",
    "\n",
    "    # rescale to (0, 1)\n",
    "    # cutoff = rgb.max()  # using max img intensity\n",
    "    # cutoff = 0.35  # using hard cutoff\n",
    "    cutoff = min(0.35, (rgb.max() + 0.35) / 2)  # hybrid cutoff\n",
    "    rgb = np.clip(rgb / cutoff, a_min=0, a_max=1)\n",
    "\n",
    "    cutoff = min(100, (100 + nl.max()) / 4)  # hybrid cutoff\n",
    "    nl = np.clip(nl / cutoff, a_min=0, a_max=1)\n",
    "\n",
    "    # Option 1: matplotlib imshow()\n",
    "    # plt.imshow(rgb, origin='lower')\n",
    "    # plt.show()\n",
    "\n",
    "    # Option 2: display(PIL.Image)\n",
    "    im255 = np.uint8(rgb * 255)\n",
    "    im = PIL.Image.fromarray(im255[::-1], mode='RGB')\n",
    "    display(im)\n",
    "\n",
    "    nl255 = np.uint8(nl * 255)\n",
    "    im = PIL.Image.fromarray(nl255[::-1], mode='L')\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXFcdES6VzNc"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=123)\n",
    "num_samples = 20\n",
    "\n",
    "surveys = os.listdir(DHS_PROCESSED_FOLDER)\n",
    "dhs_csv = pd.read_csv(DHS_INPUT_CSV_PATH, index_col='DHSID_EA')\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # sample a survey\n",
    "    survey = rng.choice(surveys)\n",
    "\n",
    "    # sample a npz file\n",
    "    npz_filenames = os.listdir(os.path.join(DHS_PROCESSED_FOLDER, survey))\n",
    "    npz_filename = rng.choice(npz_filenames)\n",
    "    npz_path = os.path.join(DHS_PROCESSED_FOLDER, survey, npz_filename)\n",
    "    print(f'===== {npz_path} =====')\n",
    "\n",
    "    dhsid_ea = os.path.splitext(os.path.basename(npz_path))[0]\n",
    "    lat, lon, year = dhs_csv.loc[dhsid_ea, ['lat', 'lon', 'year']]\n",
    "    print(lat, lon, year)\n",
    "\n",
    "    # display the RGB bands from the NPZ\n",
    "    npz_viz(npz_path)\n",
    "\n",
    "    # compare against the expected GEE image\n",
    "    ee_viz(lat, lon, year)\n",
    "\n",
    "    if i != num_samples - 1:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_csv.loc[\"PE-2004-5#-00000969\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uiqpllCrTaD"
   },
   "source": [
    "## Create final labels CSV file\n",
    "\n",
    "Some clusters in the input CSV file might not have a downloaded image. This section removes the clusters without images and outputs a final labels CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDjDssuIrSqW"
   },
   "outputs": [],
   "source": [
    "def create_final_labels(labels_path: str,\n",
    "                        export_log_path: str,\n",
    "                        index_col: str,\n",
    "                        final_csv_path: str) -> None:\n",
    "    '''\n",
    "    Args\n",
    "    - csv_path: str, path to CSV of DHS or LSMS cluster labels\n",
    "    - export_log_path: str, path to CSV log of processing TFRecords\n",
    "    - index_col: str, name of column in CSV to use as unique index\n",
    "    - final_csv_path: str, path to save final labels CSV\n",
    "    '''\n",
    "    export_log = pd.read_csv(export_log_path)\n",
    "    export_log.set_index(index_col, verify_integrity=True, inplace=True)\n",
    "\n",
    "    labels = pd.read_csv(labels_path)\n",
    "    labels.set_index(index_col, verify_integrity=True, inplace=True)\n",
    "\n",
    "    assert labels.index.isin(export_log.index).all()\n",
    "\n",
    "    failed_exports = export_log[export_log['status'] != 'processed'].index\n",
    "    num_failed_labels = labels.index.isin(failed_exports).sum()\n",
    "    if num_failed_labels > 0:\n",
    "        print(f'Failed to download images for {num_failed_labels} clusters.')\n",
    "        print('Removing those clusters to create final labels CSV.')\n",
    "\n",
    "        success_exports = export_log[export_log['status'] == 'processed']\n",
    "        final_labels = success_exports.merge(\n",
    "            labels, how='inner', left_index=True, right_index=True)\n",
    "        del final_labels['status']\n",
    "        final_labels.to_csv(final_csv_path)\n",
    "    else:\n",
    "        print('Images were exported for all labels!')\n",
    "        shutil.copy2(labels_path, final_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_final_labels(labels_path=DHS_INPUT_CSV_PATH,\n",
    "                    export_log_path='dhs_tfrecords_export_log.csv',\n",
    "                    index_col='DHSID_EA',\n",
    "                    final_csv_path=DHS_FINAL_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_final_labels(labels_path=LSMS_INPUT_CSV_PATH,\n",
    "                    export_log_path='lsms_tfrecords_export_log.csv',\n",
    "                    index_col='DHSID_EA',\n",
    "                    final_csv_path=LSMS_FINAL_CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uvX-1XUsrD-",
    "tags": []
   },
   "source": [
    "## Tar and gzip the npz files\n",
    "\n",
    "The `.tar.gz` files are sharded to take up ~20GiB (20 * 2^30 bytes). The files are sharded such that all `.npz` files from a country are in the same shard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pprint import pprint\n",
    "\n",
    "# use `du` to estimate size of npzs from each survey\n",
    "# - this overcounts, because there are some processed npzs that might not get\n",
    "#   get included in the final verison\n",
    "folder_sizes = !du -h -m -d 1 dhs_npzs/* | sed 's/\\t/,/g'\n",
    "sizes = pd.read_csv(StringIO('\\n'.join(folder_sizes)), names=['MiB', 'folder'])\n",
    "sizes['country'] = sizes['folder'].str[9:11]\n",
    "sizes_by_country = sizes.groupby('country')['MiB'].sum().sort_index().to_frame()\n",
    "display(sizes_by_country.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine country grouping\n",
    "max_size = 20 * 2**10   # in MiB\n",
    "cum_size = 0\n",
    "groups = []  # list of ([list of country codes], size in MiB)\n",
    "group = []\n",
    "for country in sorted(sizes_by_country.index):\n",
    "    size = sizes_by_country.loc[country, 'MiB']\n",
    "    if cum_size > 0 and cum_size + size > max_size:\n",
    "        groups.append((group, cum_size))\n",
    "        cum_size = 0\n",
    "        group = []\n",
    "\n",
    "    cum_size += size\n",
    "    group.append(country)\n",
    "\n",
    "groups.append((group, cum_size))\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each group, create a list of files to tar-gzip\n",
    "dhs_final = pd.read_csv(DHS_FINAL_CSV_PATH)\n",
    "dhs_final['survey'] = dhs_final['DHSID_EA'].str[:10]\n",
    "for i, group in enumerate(groups):\n",
    "    cnames = group[0]\n",
    "    files_list = dhs_final.loc[dhs_final['cname'].isin(cnames), ['survey', 'DHSID_EA']]\n",
    "    files_list = files_list['survey'] + '/' + files_list['DHSID_EA'] + '.npz'\n",
    "    files_list.sort_values(inplace=True)\n",
    "    display(files_list)\n",
    "    files_list.to_csv(f'dhs_tar_list_{cnames[0]}_{cnames[-1]}.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EH72DkFrDSZv"
   },
   "outputs": [],
   "source": [
    "# !tar -czvf dhs_AL_DR.tar.gz -C dhs_npzs -T dhs_tar_list_AL_DR.txt\n",
    "# !tar -czvf dhs_EG_HT.tar.gz -C dhs_npzs -T dhs_tar_list_AL_DR.txt\n",
    "# !tar -czvf dhs_IA_IA.tar.gz -C dhs_npzs -T dhs_tar_list_AL_DR.txt\n",
    "# !tar -czvf dhs_ID_MZ.tar.gz -C dhs_npzs -T dhs_tar_list_AL_DR.txt\n",
    "# !tar -czvf dhs_NG_SZ.tar.gz -C dhs_npzs -T dhs_tar_list_AL_DR.txt\n",
    "# !tar -czvf dhs_TD_ZW.tar.gz -C dhs_npzs -T dhs_tar_list_AL_DR.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract the tar.gz files\n",
    "# !tar -xzvf dhs_AL_DR.tar.gz -C <output_dir>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4fdHjqEv9g7"
   },
   "source": [
    "## Calculate Mean and Std-Dev for Each Band\n",
    "\n",
    "The means and standard deviations calculated here are saved as constants in `sustainbench/datasets/poverty_dataset.py` for `_MEANS_DHS`, `_STD_DEVS_DHS`, `_MEANS_LSMS`, and `_STD_DEVS_LSMS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def calculate_band_means(path_and_year) -> tuple[np.ndarray, np.ndarray, int]:\n",
    "    '''\n",
    "    Args\n",
    "    - path_year: tuple (path, year)\n",
    "      - path: str, path to npz file containing single entry 'x'\n",
    "        representing a (C, H, W) image\n",
    "      - year: int\n",
    "\n",
    "    Returns: (means, year)\n",
    "    - sums: np.ndarray, shape [C], sum of values for each band\n",
    "    - sum_sqs: np.ndarray, shape [C], sum of squares of values for each band\n",
    "    - year: int\n",
    "    '''\n",
    "    npz_path, year = path_and_year\n",
    "    img = np.load(npz_path)['x']\n",
    "    sums = np.sum(img, axis=(1, 2), dtype=np.float64)\n",
    "    sum_sqs = np.sum(img ** 2, axis=(1, 2), dtype=np.float64)\n",
    "    return sums, sum_sqs, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOecvs9Vv9g8"
   },
   "outputs": [],
   "source": [
    "dhs_final = pd.read_csv(DHS_FINAL_CSV_PATH, index_col='DHSID_EA')\n",
    "dhs_final['path'] = (\n",
    "    DHS_PROCESSED_FOLDER + '/' +\n",
    "    dhs_final.index.str[:10] + '/' +\n",
    "    dhs_final.index + '.npz'\n",
    ")\n",
    "path_years = dhs_final[['path', 'year']].apply(tuple, axis=1)\n",
    "\n",
    "sums_dmsp = []\n",
    "sum_sqs_dmsp = []\n",
    "sums_viirs = []\n",
    "sum_sqs_viirs = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=30) as pool:\n",
    "    inputs = path_years\n",
    "    futures = pool.map(calculate_band_means, inputs)\n",
    "    for sums, sum_sqs, year in tqdm(futures, total=len(inputs)):\n",
    "        if year < 2012:\n",
    "            sums_dmsp.append(sums)\n",
    "            sum_sqs_dmsp.append(sum_sqs)\n",
    "        else:\n",
    "            sums_viirs.append(sums)\n",
    "            sum_sqs_viirs.append(sum_sqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_all = np.stack(sums_dmsp + sums_viirs)\n",
    "sum_sqs_all = np.stack(sum_sqs_dmsp + sum_sqs_viirs)\n",
    "\n",
    "sums_dmsp = np.stack(sums_dmsp)\n",
    "sum_sqs_dmsp = np.stack(sum_sqs_dmsp)\n",
    "sums_viirs = np.stack(sums_viirs)\n",
    "sum_sqs_viirs = np.stack(sum_sqs_viirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate means\n",
    "num_pixels_per_img = 255 * 255\n",
    "band_means = sums_all.mean(axis=0) / num_pixels_per_img\n",
    "dmsp_mean = sums_dmsp[:, -1].mean() / num_pixels_per_img\n",
    "viirs_mean = sums_viirs[:, -1].mean() / num_pixels_per_img\n",
    "\n",
    "MEANS = {\n",
    "    band: np.float32(band_means[i])\n",
    "    for i, band in enumerate(REQUIRED_BANDS)\n",
    "}\n",
    "MEANS['DMSP'] = dmsp_mean\n",
    "MEANS['VIIRS'] = viirs_mean\n",
    "display(MEANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard deviations\n",
    "# population std-dev\n",
    "# = sqrt( E[X**2] - E[X]**2 )\n",
    "# = sqrt( sum_sqs / N - mean**2 )\n",
    "band_sd = np.sqrt( sum_sqs_all.mean(axis=0) / num_pixels_per_img - band_means**2 )\n",
    "dmsp_sd = np.sqrt( sum_sqs_dmsp[:, -1].mean() / num_pixels_per_img - dmsp_mean**2 )\n",
    "viirs_sd = np.sqrt( sum_sqs_viirs[:, -1].mean() / num_pixels_per_img - viirs_mean**2 )\n",
    "\n",
    "STD_DEVS = {\n",
    "    band: np.float32(band_sd[i])\n",
    "    for i, band in enumerate(REQUIRED_BANDS)\n",
    "}\n",
    "STD_DEVS['DMSP'] = dmsp_sd\n",
    "STD_DEVS['VIIRS'] = viirs_sd\n",
    "display(STD_DEVS)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "12jeEXcPv9gs",
    "ijWmOpSLVUSZ"
   ],
   "name": "1_process_tfrecords.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:sustainbench_create] *",
   "language": "python",
   "name": "conda-env-sustainbench_create-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
